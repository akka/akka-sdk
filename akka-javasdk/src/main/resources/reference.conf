# This is the reference config file that contains the default settings.
# Make your edits/overrides in your application.conf.

akka.javasdk {
  dev-mode {
    # default false, but set to true by maven when running locally
    enabled = false

    # the port it will use when running locally
    http-port = 9000

    # defaults to empty, but maven will set akka.javasdk.dev-mode.project-artifact-id to ${project.artifactId}
    # this is only filled in dev-mode, in prod the name will be the one chosen when the service is created
    # users can override this in their application.conf
    service-name = ""
    service-name =${?akka.javasdk.dev-mode.project-artifact-id}


    eventing {
      # Valid options are: "none", "/dev/null", "logging", "google-pubsub", "kafka",  "google-pubsub-emulator" and "eventhubs"
      support = "none"

      # The configuration for kafka brokers
      kafka {
        # One or more bootstrap servers, comma separated.
        bootstrap-servers = "localhost:9092"

        # Supported are
        # NONE (for easy local/dev mode with no auth at all)
        # PLAIN (for easy local/dev mode - plaintext, for non dev-mode TLS)
        # SCRAM-SHA-256 and SCRAM-SHA-512 (TLS)
        auth-mechanism = "NONE"
        auth-username = ""
        auth-password = ""
        broker-ca-pem-file = ""
      }
    }

    acl {
      # Whether ACL checking is enabled
      enabled = true
    }

    persistence {
      # Whether persistence is enabled
      enabled = false
    }
  }

  testkit {
    # The port used by the testkit when running integration tests
    http-port = 39390
  }

  agent {
    # The default model provider that is used if an Agent doesn't specify a specific model.
    # References a config section for the model provider, such as anthropic or openai.
    model-provider = ""

    
    # Configuration for Anthropic's large language models
    anthropic {
      # The provider name, must be "anthropic"
      provider = "anthropic"
      # The API key for authentication with Anthropic's API
      api-key = ""
      # Environment variable override for the API key
      api-key = ${?ANTHROPIC_API_KEY}
      # The name of the model to use, e.g. "claude-2" or "claude-instant-1"
      model-name = ""
      # Optional base URL override for the Anthropic API
      base-url = ""
      # Controls randomness in the model's output (0.0 to 1.0)
      temperature = NaN
      # Nucleus sampling parameter (0.0 to 1.0). Controls text generation by
      # only considering the most likely tokens whose cumulative probability
      # exceeds the threshold value. It helps balance between diversity and
      # quality of outputs—lower values (like 0.3) produce more focused,
      # predictable text while higher values (like 0.9) allow more creativity
      # and variation.
      top-p = NaN
      # Top-k sampling parameter (-1 to disable).
      # Top-k sampling limits text generation to only the k most probable
      # tokens at each step, discarding all other possibilities regardless
      # of their probability. It provides a simpler way to control randomness,
      # smaller k values (like 10) produce more focused outputs while larger
      # values (like 50) allow for more diversity.
      top-k = -1
      # Maximum number of tokens to generate (-1 for model default)
      max-tokens = -1
    }

    googleai-gemini {
      # The provider name, must be "googleai-gemini"
      provider = "googleai-gemini"
      # The API key for authentication with Google AI Gemini's API
      api-key = ""
      # Environment variable override for the API key
      api-key = ${?GOOGLE_AI_GEMINI_API_KEY}
      # The name of the model to use, e.g. "gemini-2.0-flash", "gemini-1.5-flash", "gemini-1.5-pro" or "gemini-1.0-pro"
      model-name = ""
      # Controls randomness in the model's output (0.0 to 1.0)
      temperature = NaN
      # Nucleus sampling parameter (0.0 to 1.0). Controls text generation by
      # only considering the most likely tokens whose cumulative probability
      # exceeds the threshold value. It helps balance between diversity and
      # quality of outputs—lower values (like 0.3) produce more focused,
      # predictable text while higher values (like 0.9) allow more creativity
      # and variation.
      top-p = NaN
      # Maximum number of tokens to generate (-1 for model default)
      max-output-tokens = -1
    }

    # Configuration for Ollama large language models
    ollama {
      # The provider name, must be "ollama"
      provider = "ollama"
      # Ollama server base url
      base-url = "http://localhost:11434"
      # One of the models installed in the Ollama server
      model-name = ""
      # Controls randomness in the model's output (0.0 to 1.0)
      temperature = NaN
      # Nucleus sampling parameter (0.0 to 1.0). Controls text generation by
      # only considering the most likely tokens whose cumulative probability
      # exceeds the threshold value. It helps balance between diversity and
      # quality of outputs—lower values (like 0.3) produce more focused,
      # predictable text while higher values (like 0.9) allow more creativity
      # and variation.
      top-p = NaN
    }

    # Configuration for OpenAI's large language models
    openai {
      # The provider name, must be "openai"
      provider = "openai"
      # The API key for authentication with OpenAI's API
      api-key = ""
      # Environment variable override for the API key
      api-key = ${?OPENAI_API_KEY}
      # The name of the model to use, e.g. "gpt-4" or "gpt-3.5-turbo"
      model-name = ""
      # Optional base URL override for the OpenAI API
      base-url = ""
      # Controls randomness in the model's output (0.0 to 1.0)
      temperature = NaN
      # Nucleus sampling parameter (0.0 to 1.0). Controls text generation by
      # only considering the most likely tokens whose cumulative probability
      # exceeds the threshold value. It helps balance between diversity and
      # quality of outputs—lower values (like 0.3) produce more focused,
      # predictable text while higher values (like 0.9) allow more creativity
      # and variation.
      top-p = NaN
      # Maximum number of tokens to generate (-1 for model default)
      max-tokens = -1
    }

    # Configuration for the session history (memory) between an Agent and the LLM model
    memory {
      # By default, the session history is turned on for all agents. It can be turned off with this setting.
      enabled = true

      # The maximum size of the memory window for the session history.
      # This is calculated as the sum of all messages content length in bytes.
      # Once the limit is reached, older messages will be automatically removed in a FIFO approach.
      # The default value is 510 KiB and this is actually the maximum value allowed. This is due to the fact that these
      # messages might be routed around the Akka cluster and as such some resource contraints apply.
      limited-window.max-size = 510 KiB
    }
  }

  entity {
    # When a EventSourcedEntity, KeyValueEntity or Workflow is deleted the existence of the entity is completely cleaned up after
    # this duration. The events and snapshots will be deleted later to give downstream consumers time to process all
    # prior events, including final deleted event. Default is 7 days.
    cleanup-deleted-after =  ${akka.javasdk.event-sourced-entity.cleanup-deleted-after}
  }

  delete-entity.cleanup-interval = 1 hour

  event-sourced-entity {
    # It is strongly recommended to not disable snapshotting unless it is known that
    # event sourced entities will never have more than 100 events (in which case
    # the default will anyway not trigger any snapshots)
    snapshot-every = 100

    # Deprecated, use akka.javasdk.entity.cleanup-deleted-after
    cleanup-deleted-after = 7 days
  }

  discovery {
    # By default all environment variables of the process are passed along to the runtime, they are used only for
    # substitution in the descriptor options such as topic names. To selectively pick only a few variables,
    # this setting needs to be set to false and `pass-along-env-allow` should be configured with
    # a list of variables we want to pass along.
    pass-along-env-all = true

    # By default all environment variables of the process are passed along to the runtime, they are used only for
    # substitution in the descriptor options such as topic names. This setting can
    # limit which variables are passed configuring this as a list of allowed names:
    # pass-along-env-allow = ["ENV_NAME1", "ENV_NAME2"]
    # This setting only take effect if pass-along-env-all is set to false, otherwise all env variables will be pass along.
    # To disable any environment variable pass along, this setting needs to be an empty list pass-along-env-allow = []
    # and pass-along-env-all = false
    pass-along-env-allow = []
  }

  grpc.client {
    # Specify entries for the full service DNS names to apply
    # customizations for interacting with external gRPC services.
    # The example block shows the customizations keys that are accepted:
    #
    # "com.example" {
    #   host = "192.168.1.7"
    #   port = 5000
    #   use-tls = false
    # }
  }

  telemetry {
    tracing {
      collector-endpoint = ""
      collector-endpoint = ${?COLLECTOR_ENDPOINT}
    }
  }
}
