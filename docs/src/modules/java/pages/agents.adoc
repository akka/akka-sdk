= Implementing agents

include::ROOT:partial$include.adoc[]

image:ROOT:agent.png[Agent,width=100,float=left]An Agent interacts with an AI model to perform a specific task. It is typically backed by a large language model (LLM). It maintains contextual history in a session memory, which may be shared between multiple agents that are collaborating on the same goal. It may provide function tools and call them as requested by the model.

== Identify the agent
Every component in Akka needs to be identifiable by the rest of the system. This usually involves two different forms of identification: a **component ID** an **instance ID**. We use component IDs as a way to identify the component _class_ and distinguish it from others. Instance identifiers are, as the name implies, unique identifiers for an instance of a component.

As with all other components, we supply an identifier for the component class using the `@Component` annotation.

In the case of agents, we don't supply a unique identifier for the instance of the agent. Instead, we supply an identifier for the _session_ to which the agent is bound. This lets you have multiple components with different component IDs all performing various agentic tasks within the same shared session.

[#_effect_api]
== Agent's effect API

Effects are declarative in nature. When components handle commands, they can return an `Effect`. Some components can produce only a few effects while others, such as the Agent, can produce a wide variety.

The Agent's Effect defines the operations that Akka should perform when an incoming command is handled by an Agent. These effects can be any of the following:

* declare which model will be used
* specify system messages, user messages and additional context (prompts)
* configure session memory
* define available tools
* fail a command by returning an error
* return an error message
* transform responses from a model and reply to incoming commands

For additional details, refer to xref:concepts:declarative-effects.adoc[Declarative Effects].

== Skeleton

An agent implementation has the following code structure.

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/application/MyAgent.java[MyAgent.java]
----
include::example$doc-snippets/src/main/java/com/example/application/MyAgent.java[tag=class]
----
<1> Create a class that extends `Agent`.
<2> Make sure to annotate the class with `@Component` and pass a unique identifier for this agent type.
<3> Define the command handler method.

NOTE: The `@Component` value `my-agent` is common for all instances of this agent and must be unique across the different components in the service.

An agent must have one command handler method that is public and returns `Effect<T>`, where `T` it the type of the reply. Alternatively it can return `StreamEffect` for xref:#_streaming_response[streaming responses].

Command handlers in Akka may take one or no parameters as input. If you need multiple parameters for a command, you can wrap them in a record class and pass an instance of that to the command handler as the sole parameter.

There can only be one command handler because the agent is supposed to perform one single well-defined task.

[#model]
== Configuring the model

Akka provides integration with several backend AI models, and you have to select which model to use. You can define a default model in `application.conf`:

[source,json,indent=0]
.src/main/resources/application.conf
----
include::java:example$doc-snippets/src/main/resources/application.conf[tags=agent-model-config]
----

The default model will be used if the agent doesn't specify another model. Different agents can use different models by defining the `ModelProvider` in the Agent effect:

[source,java,indent=0]
.MyAgent.java
----
include::example$doc-snippets/src/main/java/com/example/application/MyAgentMore.java[tag=model]
----
<1> Define the model provider in code.

NOTE: With `ModelProvider.fromConfig` you can define several models in configuration and use different models in different agents.


Available model providers for hosted models are:

[options="header" cols="1,1"]
|===
| Provider
| Site

| Anthropic | link:https://www.anthropic.com/[anthropic.com]
| GoogleAIGemini | link:https://gemini.google.com/[gemini.google.com]
| Hugging Face | link:https://huggingface.co[huggingface.co]
| OpenAi | link:https://openai.com/[openai.com]

|===

Additionally, these model providers for locally running models are supported:

[options="header" cols="1,1"]
|===
| Provider
| Site

| LocalAI | link:https://localai.io/[localai.io]
| Ollama | link:https://ollama.com/[ollama.com]
|===

Each model provider may have different settings and those are described in xref:model-provider-details.adoc[]

It is also possible to plug in a custom model by implementing the link:_attachments/api/akka/javasdk/agent/ModelProvider.Custom.html[`ModelProvider.Custom`] interface and use it with `ModelProvider.custom`.

== Choosing the prompt

The prompt consists of essential instructions to the model.

* The system message provides system-level instructions to the AI model that defines its behavior and context. The system message acts as a foundational prompt that establishes the AI's role, constraints, and operational parameters. It is processed before user messages and helps maintain consistent behavior throughout the interactions.
* The user message represents the specific query, instruction, or input that will be processed by the model to generate a response.

An agent that suggests real-world activities may have a prompt like:

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/application/ActivityAgent.java[ActivityAgent.java]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgent.java[tag=prompt]
----
<1> Define the system message as a constant, but it could also be a method that adapts the system message based on the request.
<2> Use the system message in the effect builder.
<3> Define the user message for the specific request, and use in the effect builder.

Keep in mind that some models have preferences in how you wrap or label user input within the system prompt and you'll need to take that into account when defining your system message.

=== Using dynamic prompts with templates

As an alternative to hard-coded prompts, there is a built-in prompt template entity. The advantage of using the prompt template entity is that you can change the prompts at runtime without restarting or redeploying the service. Because the prompt template is managed as an entity, you retain full change history.

[source,java,indent=0]
.ActivityAgent.java
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=prompt-template]
----
<1> Define the system message prompt template key.

In addition to the prompt template key you can optionally add parameters to `systemMessageFromTemplate`. Those will be used to format the template with `java.util.Formatter`.

Prompts are stored in the `PromptTemplate` xref:event-sourced-entities.adoc[Event Sourced Entity]. This is a built-in entity, automatically registered at runtime if there are any Agent components in the service. To initialize the prompt or get the current value you can use component client the same way as for any other entity.

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/api/ActivityPromptEndpoint.java[ActivityPromptEndpoint.java]
----
include::example$doc-snippets/src/main/java/com/example/api/ActivityPromptEndpoint.java[tag=prompt-template-endpoint]
----
<1> Prompt key is used as entity id.
<2> `PromptTemplate::update` update the prompt value.
<3> `PromptTemplate::get` retrieves the current prompt value.

Keeping the prompt in the Event Sourced Entity lets you see the history of all changes. It's also possible to subscribe to changes in the prompt template entity, so that you can build a xref:views.adoc[View] or react to changes in the prompt.

The following table describes all of the methods available for the `PromptTemplate` entity:

[cols="1,3", width=85%]
|===
| Method | Description

| `init` | Initializes the prompt template with a given value. If the prompt template already exists, it will not change it. Useful for setting the initial value, e.g. in the `onStartup` method of the xref:setup-and-dependency-injection.adoc#_service_lifecycle[ServiceSetup].
| `update` | Updates the prompt template with a new value. If the prompt template does not exist, it will create it. If the value is the same as the current value, it will not change it.
| `get` |  Retrieves the current value of the prompt template. If the prompt template does not exist, it will throw an exception.
| `getOptional` | Retrieves the current value of the prompt template as an `Optional`. If the prompt template does not exist, it will return an empty `Optional`.
| `delete` | Deletes the prompt template.

|===

Although the system message has a dedicated method to use the prompt template, you can also use it for the user message. In that case you have to use the component client to retrieve the current value of the prompt template and pass it as the user message.

=== Adding more context

xref:rag.adoc[] is a technique to provide additional, relevant content in the user message.

== Calling the Agent

Use the `ComponentClient` to call the agent from a Workflow, Endpoint or Consumer.

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgent.java[tag=call]
----
<1> Use `forAgent`.
<2> Define the identifier of the session that the agent participates in.

The session id is used by the xref:#session_memory[session memory], but it is also important for observability tracking and AI evaluation.

You can use a new random UUID for each call if the agent doesn't collaborate with other agents nor have a multi-step interaction with the AI model. Deciding how you manage sessions will be an important part of designing the agentic parts of your application.

For more details about the `ComponentClient`, see xref:component-and-service-calls.adoc[].

== Drive the agent from a workflow

Agents make external calls to the AI model and possibly other services, and therefore it is important to have solid error handling and durable execution steps when calling agents. In many cases it is a good recommendation to call agents from a xref:workflows.adoc[Workflow]. The workflow will automatically execute the steps in a reliable and durable way. This means that if a call in a step fails, it will be retried until it succeeds or the retry limit of the recovery strategy is reached and separate error handling can be performed. The state machine of the workflow is durable, which means that if the workflow is restarted for some reason it will continue from where it left off, i.e. execute the current non-completed step again.

A workflow will typically orchestrate several agents, which collaborate in achieving a common goal. Even if you only have a single agent, having a workflow manage retries, failures, and timeouts can be invaluable.

We will look more at xref:#multi_agent[multi-agent systems], but let's start with a workflow for the single activities agent.

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/application/ActivityAgentManager.java[ActivityAgentManager.java]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentManager.java[tag=all]
----
<1> Extend `Workflow`.
<2> The state can hold intermediate and final results, and it is durable.
<3> Inject the `ComponentClient`, which will be used when calling the agent.
<4> This workflow only has two command handler methods. One that starts the workflow with the initial user request,
<5> and one to retrieve the final answer.
<6> Define the workflow configuration.
<7> The step that calls the `ActivityAgent`
<8> Call the agent with the `ComponentClient`
<9> Store the result from the agent.
<10> The workflow corresponds to an agent session.

The workflow itself will be instantiated by making a call to the `start` method from an endpoint or a consumer.

Keep in mind that AI requests are typically slow (many seconds), and you need to define the workflow timeouts accordingly. This is specified in the workflow step definition with:

[source,java,indent=0]
----
.stepConfig(ActivityAgentManager::suggestActivities, ofSeconds(60))
----

Additionally, you should define a workflow recovery strategy so that it doesn't retry failing requests infinitely. This is specified in the workflow definition with:

[source,java,indent=0]
----
.defaultStepRecovery(maxRetries(2).failoverTo(ActivityAgentManager::error))
----

More details in xref:workflows.adoc#_error_handling[Workflow timeouts and recovery strategy].

=== Human in the loop

You often need a human-in-the-loop to integrate human oversight into the AI's decision-making process. A workflow can be paused, waiting for user input. When the approval command is received, the workflow can continue from where it left off and transition to the next step in the agentic process.

See xref:workflows.adoc#_pausing_workflow[how to pause a workflow].

[#session_memory]
== Managing session memory

Session Memory provides a history mechanism that enables agents to maintain context across multiple interactions. This feature is essential for building agents that can remember previous exchanges with users, understand context, and provide coherent responses over time.

When an agent interacts with an AI model, both the user message and the AI response are automatically stored in the session memory. These messages are then included as additional context in subsequent requests to the model, allowing it to reference previous parts of the interaction.

The session memory is:

* Identified by a session ID that links related interactions
* Shared between multiple agents if they use the same session ID
* Persisted as an event-sourced entity
* Automatically managed by the Agent

=== Session memory configuration

By default, session memory is enabled for all agents. You can configure it globally in your `application.conf`:

[source,conf,indent=0]
----
akka.javasdk.agent.memory {
  enabled = true
  limited-window {
    max-size = 156KiB # max history size before oldest message start being removed
  }
}
----

Or you can configure memory behavior for specific agent interactions using the `MemoryProvider` API.

Example with `limitedWindow` memory provider:
[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/MyAgentMore.java[tag=read-last]
----


Example disabling session memory for the agent:
[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/MyAgentMore.java[tag=no-memory]
----


=== Different memory providers

The link:_attachments/api/akka/javasdk/agent/MemoryProvider.html[`MemoryProvider`] interface allows you to control how session memory behaves:

* `MemoryProvider.none()` - Disables both reading from and writing to session memory
* `MemoryProvider.limitedWindow()` - Configures memory with options to, e.g.:
  ** Setup **read only** memory, in which the agent reads the memory but does not allow write any interactions to it. This is ideal for multi-agent sessions where some agents can store memory and others can't.
  ** Setup **write only** memory, in which the agent register the interactions to the session memory but does not take those in consideration when processing the user message.
  ** Limit the amount of messages used as context in each interaction, i.e. use only the last N number of messages for context (good for token usage control).
* `MemoryProvider.custom()` - Allows you to provide a custom implementation for the `SessionMemory` interface and store the session memory externally in a database / service of your preference.

=== Accessing session memory

The default implementation of Session Memory is backed by a regular xref:java:event-sourced-entities.adoc[Event Sourced Entity] called `SessionMemoryEntity`, which allows you to interact directly with it as you would do with any other entities in your application. This includes the possibility to directly modify or access it through the `ComponentClient` but also the ability to subscribe to changes in the session memory, as shown below:

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/application/SessionMemoryConsumer.java[SessionMemoryConsumer.java]
----
include::example$doc-snippets/src/main/java/com/example/application/SessionMemoryConsumer.java[tag=consumer]
----

This can be useful for more granular control over token usage but also to allow external integrations and analytics over these details.

=== Compaction

You can update the session memory to reduce the size of the history. One technique is to let an LLM summarize the interaction history and use the new summary instead of the full history. Such agent can look like this:

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/application/CompactionAgent.java[CompactionAgent.java]
----
include::example$doc-snippets/src/main/java/com/example/application/CompactionAgent.java[tag=compaction]
----
<1> Instructions to create the summary of user and AI messages and result as JSON.
<2> The full history from the `SessionMemoryEntity`.
<3> Format and concatenate the messages.
<4> The `CompactionAgent` itself doesn't need any session memory.

One way to trigger compaction is to use a consumer of the session memory events and call the `CompactionAgent` from that consumer when a threshold is exceeded.

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/application/SessionMemoryConsumer.java[SessionMemoryConsumer.java]
----
include::example$doc-snippets/src/main/java/com/example/application/SessionMemoryConsumer.java[tag=compaction]
----
<1> The AiMessageAdded has the total size of the history.
<2> Retrieve the full history from the `SessionMemoryEntity`.
<3> Call the agent to make the summary.
<4> Store the summary as the new compacted history in the `SessionMemoryEntity`.
<5> To support concurrent updates, the `sequenceNumber` of the retrieved history is included in the `CompactionCmd`.

== Structured responses

Many LLMs support generating outputs in a structured format, typically JSON. You can easily map such output to Java objects using the effect API.

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=structured-response]
----
<1> Instruct the model to return a structured response in JSON format.
<2> `Activity` record is used to map the JSON response to a Java object.
<3> Use the `responseAs` method to specify the expected response type.
<4> Sometimes the model may not return a valid JSON, so you can use `onFailure` to provide a fallback value in case of parsing exception.

Some models, such as OpenAI and Google Gemini, have specific support for structured model responses according to a given JSON schema. To automatically include a JSON schema that corresponds to the response type you can use `responseConformsTo` instead of `responseAs`.

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=structured-response-schema]
----
<1> Instructions to the model doesn't have to include details about the JSON response format.
<2> `Activity` record is used to map the JSON response to a Java object. It can optionally have `akka.javasdk.annotations.Description` of the fields, which will be included in the JSON schema.
<3> Use the `responseConformsTo` method  to specify the expected response type, which is also used for creating the JSON schema.

If you still don't get expected JSON responses from the model, you can combine those two approaches of both including the JSON schema and giving instructions about the format in the system message.

== Handling failures

The `onFailure` method in the agent's effect API provides comprehensive error handling capabilities for various types of failures that can occur during model processing. This allows you to implement robust fallback strategies and provide meaningful responses even when things go wrong.

=== Types of exceptions handled

The `onFailure` method can handle the following types of exceptions:

* **Model-related exceptions:**
  ** `ModelException` - General model processing failures
  ** `RateLimitException` - API rate limiting exceeded
  ** `ModelTimeoutException` - Model request timeout
  ** `UnsupportedFeatureException` - Unsupported model features
  ** `InternalServerException` - Internal service errors

* **Tool execution exceptions:**
  ** `ToolCallExecutionException` - Function tool execution errors
  ** `McpToolCallExecutionException` - MCP tool execution errors
  ** `ToolCallLimitReachedException` - Tool call limit exceeded

* **Response processing exceptions:**
  ** `JsonParsingException` - Response parsing failures (as shown in structured responses)

* **Unknown exceptions:**
  ** `RuntimeException` - For any unexpected errors that don't fall into the above categories

Apart from the listed specific exceptions, users can still encounter `RuntimeException` instances that wrap unexpected errors. Therefore, when handling errors in the `onFailure` method, it's recommended to always include a `default` case to handle any unknown exception types gracefully.

=== Implementing fallback strategies

You can use the `onFailure` method to implement different recovery strategies based on the type of exception:

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=error-handling]
----
<1> Use pattern matching to handle different exception types appropriately.
<2> Handle specific known exceptions with meaningful fallback responses.
<3> For unknown or unexpected exceptions, define a default matching branch providing a generic fallback response.

This approach ensures your agents remain resilient and can provide meaningful responses even when encountering various types of failures during model interaction.

[#tools]
== Extending agents with function tools

You may frequently hear people say things like "the LLM can make a call" or "the LLM can use a tool". While these statements get the point across, they're not entirely accurate. In truth, the agent will tell the LLM which _tools_ are available for use. The LLM then determines from the prompt which tools it needs to call and with which parameters.

The Agent will then in turn execute the tool requested by the LLM, incorporate the tool results into the session context, and then send a new prompt. This will continue in a loop until the LLM no longer indicates it needs to invoke a tool to perform its task.

There are three ways to add function tools to your agent:

1. **Agent-defined function tools** — Define function tools directly within your agent class using the `@FunctionTool`
annotation. These are automatically registered as available tools for the current Agent.

2. **Externally defined function tools** — Explicitly register external objects or classes containing function tools by
passing them to the `effects().tools()` method in your agent's command handler. Objects or classes passed to `effects
().tools()` must have at least one public method annotated with `@FunctionTool`.

3. **Tools defined by remote MCP servers** – Register remote MCP servers to let the agent use tools they provide.

NOTE: A class (either the agent itself or an external tool class) can have multiple methods annotated with `@FunctionTool`. Each annotated method will be registered as a separate tool that the LLM can choose to invoke based on the task requirements.

You can use either approach independently or combine them based on your needs. Let's look at a complete example showing both approaches:

[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/WeatherAgent.java[WeatherAgent.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/WeatherAgent.java[tag=function-tool]
----
<1> The `WeatherService` providing a function tool is injected into the agent (see
xref:setup-and-dependency-injection.adoc#_custom_dependency_injection[DependencyProvider]).
<2> We explicitly register the `weatherService` using the `tools()` method to make its method available as a tool for
 the current Agent.
<3> We define a simple tool directly in the agent class using the `@FunctionTool` annotation, which is implicitly registered. Note that since this method is defined in the agent itself, it can even be a private method.

The `WeatherService` is an interface with a method annotated with `@FunctionTool`. A concrete implementation of this
interface is provided by `{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/WeatherServiceImpl.java[WeatherServiceImpl]` class.
This class is made available for injection in the service setup using a xref:setup-and-dependency-injection.adoc#_custom_dependency_injection[DependencyProvider].

[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/WeatherService.java[WeatherService.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/WeatherService.java[tag=function-tool]
----

<1> Annotate method with `@FunctionTool` and provide a clear description of what it does.
<2> Parameters can be documented with the `@Description` annotation to help the LLM understand how to use them.
<3> The date parameter is optional. The LLM may call `getCurrentDate` first or call this method without a date, depending on the user query.


[NOTE]
====
LLMs are all about context. The more context you can provide, the better the results.
Both `@FunctionTool` and `@Description` annotations are used to provide context to the LLM about the tool function and its parameters.
The better the context, the better the LLM can understand what the tool function does and how to use it.
====

In this example, the agent has access to both:

* The `getCurrentDate()` method defined within the agent class (implicitly registered via annotation)
* The `getWeather()` method defined in the `WeatherService` interface (explicitly registered via the `.tools()` method)

=== Sharing function tools across agents

Function tools defined in external classes can be shared and reused across multiple agents. This approach promotes code reusability and helps maintain a consistent behavior for common functionalities.

When a tool like `WeatherService` is shared across multiple agents:

* Each agent can register the same tool but use it in different contexts
* The tool behavior remains consistent, but how and when agents invoke it may differ based on their specific tasks
* Agents provide different system prompts that influence how the LLM decides to use the shared tool


=== Lazy initialization of tool classes

In the example above, we pass an instance of `WeatherService` to the `tools()` method. Alternatively, you can pass the `Class` object instead:

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/application/LazyWeatherAgent.java[LazyWeatherAgent
.java]
----
include::example$doc-snippets/src/main/java/com/example/application/LazyWeatherAgent.java[tag=function-tool]
----
<1> The WeatherService is passed as a `Class` instead of an instance. It will be instantiated when the agent needs to use it.

When you pass a `Class` instead of an instance, the class is only instantiated when the agent actually needs to use the tool.

For this approach to work, you must register the class with a xref:setup-and-dependency-injection.adoc#_custom_dependency_injection[DependencyProvider] in your service setup. The DependencyProvider is responsible for creating and managing instances of these classes when they're needed. This gives you complete control over how tool dependencies are instantiated and managed throughout your application.

=== Using tools from remote MCP servers ===

xref:mcp-endpoints.adoc[Akka MCP endpoints] declared in other services, or third party MCP services can be added to
the agent. By default, all tools provided by each added remote MCP server are included, but it is possible to filter
available tools from each server based on their name.

It is also possible to intercept, modify, or deny MCP tool requests, or their responses by defining a `RemoteMcpTools.ToolInterceptor`.

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/application/RemoteMcpWeatherAgent.java[RemoteMcpWeatherAgent.java]
----
include::example$doc-snippets/src/main/java/com/example/application/RemoteMcpWeatherAgent.java[tag=mcp-function-tool]
----
<1> For MCP endpoints in other Akka services, use HTTP and the deployed service name
<2> For third party MCP servers use the fully qualified host name and make sure to use HTTPS as the requests will
    go over the public internet.
<3> Custom headers to pass along can be defined
<4> As well as filters of what tools to allow.


When using MCP endpoints in other Akka services, the service ACLs apply just like for xref:http-endpoints.adoc[HTTP endpoints] and xref:grpc-endpoints.adoc[gRPC endpoints].

[#configuring_tool_call_limits]
=== Configuring tool call limits

Inside a single request/response cycle, an LLM can successively request the agent to call functions tools or MCP tools. After analyzing the result of a call, the LLM might decide to request another call to gather more context. The `akka.javasdk.agent.max-tool-call-steps` setting limits how many such steps may occur between a user request and the final AI response.

By default, this value is set to 100. You can adjust this in your configuration:

[source,hocon,indent=0]
.application.conf
----
# Increase the limit to allow more tool calls
akka.javasdk.agent.max-tool-call-steps = 150
----

== Use ComponentClient in an agent

xref:java:setup-and-dependency-injection.adoc#_dependency_injection[Dependency injection] can be used in an
Agent. For example, injecting the `ComponentClient` to be able to enrich the request to the AI model with
information from entities or views may look like this:

[source,java,indent=0]
.ActivityAgent.java
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=di]
----
<1> Inject the `ComponentClient` as a constructor parameter.
<2> Retrieve preferences from an entity.
<3> Enrich the user message with the preferences.

This also illustrates the important point that the context of the request to the AI model can be built from additional information in the service and doesn't only have to come from the session memory.

The ability to reach into the rest of a distributed Akka application to _augment_ requests makes behavior like Retrieval Augmented Generation (RAG) simple and less error prone than doing things manually without Akka.

== Streaming responses

In AI chat applications, you've seen how responses are displayed word by word as they are generated. There are a few reasons for this. The first is that LLMs are _prediction_ engines. Each time a token (usually a word) is streamed to the response, the LLM will attempt to _predict_ the next word in the output. This causes the small delays between words.

The other reason why responses are streamed is that it can take a very long time to generate the full response, so the user experience is much better getting the answer as a live stream of tokens. To support this real-time user experience, the agent can stream the model response tokens to an endpoint. These tokens can then be pushed to the client using server-sent events (SSE).

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=stream-tokens]
----
<1> The method returns `StreamEffect` instead of `Effect<T>`.
<2> Use the `streamEffects()` builder.

Consuming the stream from an HTTP endpoint:

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=stream-endpoint]
----
<1> Use `tokenStream` of the component client, instead of `method`,
<2> and invoke it with `source` to receive a stream of tokens.
<3> Return the stream of tokens as SSE.

The returned stream is a `Source<String, NotUsed>`, i.e. the tokens are always text strings.

The granularity of a token varies by AI model, often representing a word or a short sequence of characters. To reduce the overhead of sending each token as a separate SSE, you can group multiple tokens together using the Akka streams `groupWithin` operator.

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=stream-group]
----
<1> Group at most 20 tokens or within 100 milliseconds, whatever happens first.
<2> Concatenate the list of string into a single string.
<3> Return the stream of grouped tokens as SSE.

NOTE: Token streams are designed for direct agent calls from an endpoint. You can't use a token stream when you have an intermediate workflow between the endpoint and the agent.

[#multi_agent]
== Orchestrating multiple agents

A single agent performs one well-defined task. Several agents can collaborate to achieve a common goal. The agents should be orchestrated from a predefined workflow or a dynamically created plan.

=== Using a predefined workflow

Let's first look at how to define a workflow that orchestrates several agents in a predefined steps. This is similar to the xref:#_drive_the_agent_from_a_workflow[`ActivityAgentManager`] that was illustrated above, but it uses both the `WeatherAgent` and the `ActivityAgent`. First it retrieves the weather forecast and then it finds suitable activities.

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/AgentTeamWorkflow.java[tag=all]
----
<1> The workflow starts by asking for the weather forecast.
<2> Weather forecast is retrieved by the `WeatherAgent`, which must extract the location and date from the user query.
<3> The forecast is stored in the state of the workflow.
<4> The forecast is included in the request to the `ActivityAgent`.
<5> The final result is stored in the workflow state.

In image:concepts:steps-4.svg[width=20] we explicitly include the forecast in the request to the `ActivityAgent`. That is not strictly necessary because the agents share the same session memory and thereby the `ActivityAgent` will already have the weather forecast in the context that is sent to the AI model.

The workflow will automatically execute the steps in a reliable and durable way. This means that if a call in a step fails, it will be retried until it succeeds or the retry limit of the recovery strategy is reached and separate error handling can be performed. The state machine of the workflow is durable, which means that if the workflow is restarted for some reason it will continue from where it left off, i.e. execute the current non-completed step again.

=== Creating dynamic plans

To create a more flexible and autonomous agentic system you want to analyze the problem and dynamically come up with a plan. The agentic system should identify the tasks to achieve the goal by itself. Decide which agents to use and in which order to execute them. Coordinate input and output between agents and adjust the plan along the way.

There are several approaches for the planning, such as using deterministic algorithms or using AI also for the planning. We will look at how we can use AI for analyzing a request, selecting agents and in which order to use them.

We split the planning into two steps and use two separate agents for these tasks. It's not always necessary to use several steps for the planning. You have to experiment with what works best for your problem domain.

. Select agents that are useful for a certain problem.
. Decide in which order to use the agents and give each agent precise instructions for its task.

The `SelectorAgent` decides which agents to use:

[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/SelectorAgent.java[SelectorAgent.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/SelectorAgent.java[tag=class]
----
<1> The `AgentRegistry` contains information about all agents.
<2> Select the agents with the role `"worker"`.
<3> Detailed instructions and include descriptions (as json) of the agents.

The information about the agents in the `AgentRegistry` comes from the `@Component` and `@AgentDescription` annotations. When using it for planning like this it is important that the agents define those descriptions that the LLM can use to come up with a good plan.

The `WeatherAgent` has:
[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/WeatherAgent.java[WeatherAgent.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/WeatherAgent.java[tag=description]
----

The `ActivityAgent` has:
[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/ActivityAgent.java[ActivityAgent.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/ActivityAgent.java[tag=description]
----

Note that in image:concepts:steps-2.svg[width=20] of the `Selector` we retrieve a subset of the agents with a certain role. This role is also defined in the `@AgentDescription` annotation.

The result from the `Selector` is a list of agent ids:
[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/domain/AgentSelection.java[AgentSelection.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/domain/AgentSelection.java[tag=all]
----

After selecting agents, we use a `PlannerAgent` to decide in which order to use the agents and what precise request that each agent should receive to perform its single task.

[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/PlannerAgent.java[PlannerAgent.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/PlannerAgent.java[tag=class]
----
<1> Lookup the agent information for the selected agents from the `AgentRegistry.
<2> Detailed instructions and include descriptions (as json) of the agents.

That's the two agents that perform the planning, but we also need to connect them and execute the plan. This orchestration is the job of a workflow, called `AgentTeamWorkflow`.

[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/AgentTeamWorkflow.java[AgentTeamWorkflow.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/AgentTeamWorkflow.java[tag=plan]
----
<1> It's a workflow, with reliable and durable execution.
<2> The steps are select - plan - execute - summarize.
<3> The workflow starts by selecting agents.
<4> which is performed by the `SelectorAgent`.
<5> Continue with making the actual plan
<6> which is performed by the `PlannerAgent`, using the selection from the previous step.
<7> Continue with executing the plan.
<8> Take the next task in the plan.
<9> Call the agent for the task.
<10> Continue executing the plan until no tasks are remaining.

When executing the plan and calling the agents we know the id of the agent to call, but not the agent class. It can be the `WeatherAgent` or `ActivityAgent`. Therefore, we can't use the ordinary `method` of the `ComponentClient. Instead, we use the `dynamicCall` with the id of the agent. We don't have compile time safety for those dynamic calls, but we know that these agents take a String parameter and return AgentResponse. If we used it with the wrong types, it would be a runtime exception.

[source,java,indent=0]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/AgentTeamWorkflow.java[tag=dynamicCall]
----

You find the full source code for this multi-agent sample in the link:https://github.com/akka-samples/multi-agent[akka-samples/multi-agent GitHub Repository].

== Guardrails

Guardrails can protect against harmful inputs, such as jailbreak attempts, and damaging output, such as mentions of a competitor’s product.

A specific guardrail implements the link:_attachments/api/akka/javasdk/agent/TextGuardrail.html[`TextGuardrail` interface]. It takes the input or output text as a parameter and a result if it passed the validation or not, including an explanation of why the decision was made. These results are included in metrics and traces. A guardrail can abort the interaction with the model, or only report the problem and continue anyway.

An example of a `Guardrail` implementation:

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/guardrail/ToxicGuard.java[ToxicGuard.java]
----
include::example$doc-snippets/src/main/java/com/example/guardrail/ToxicGuard.java[tag=all]
----

Guardrails are enabled by configuration, to be able to enforce at deployment time that certain guardrails are always used.

[source,conf,indent=0]
----
akka.javasdk.agent.guardrails {
  "pii guard" {                                     // <1>
    class = "com.example.guardrail.PiiGuard"       // <2>
    agent = ["planner-agent"]                       // <3>
    agent-roles = ["worker"]                        // <4>
    category = PII                                  // <5>
    use-for = ["model-request", "mcp-tool-request"] // <6>
    report-only = false                             // <7>
  }

  "toxic guard" {
    class = "com.example.guardrail.ToxicGuard"
    agent-roles = ["worker"]
    category = TOXIC
    use-for = ["model-response", "mcp-tool-response"]
    report-only = false
    search-for = "bad stuff"
  }
}
----
<1> Each configured guardrail has a unique name.
<2> Implementation class of the guardrail.
<3> Enable this guardrail for agents with these component ids.
<4> Enable this guardrail for agents with these roles.
<5> The type of validation, such as PII and TOXIC.
<6> Where to use the guardrail, such as for the model request or model response.
<7> If it didn't pass the evaluation criteria, the execution can either be aborted or continue anyway. In both cases, the result is tracked in logs, metrics and traces.

The implementation class of the guardrail is configured with the `class` property. The class must implement the link:_attachments/api/akka/javasdk/agent/TextGuardrail.html[`TextGuardrail` interface]. The class may optionally have a constructor with a link:_attachments/api/akka/javasdk/agent/GuardrailContext.html[`GuardrailContext`] parameter, which includes the name and the config section for the specific guardrail. In above code example of the `ToxicGuard` you can see how the configuration property `search-for` is read from the configuration of the `GuardrailContext` parameter.

Agents are selected by matching `agent` or `agent-role` configuration.

* `agents`: enabled for agents with these component ids, if `agents` contain `"*"` the guardrail is enabled for all agents
* `agent-roles`: enabled for agents with these roles, if agent-roles contain `"*"` the guardrail is enabled for all agents that has a role, but not for agents without a role

If both `agents` and `agent-roles` are defined it's enough that one of them matches to enable the guardrail for an agent.

This role is defined in the `@AgentDescription` annotation.

The name and the category are reported in logs, metrics and traces. The `category` should classify the type of validation. It can be any value, but a few recommended categories are JAILBREAK, PROMPT_INJECTION, PII, TOXIC, HALLUCINATED, NSFW, FORMAT.

The guardrail can be enabled for certain inputs or outputs with the `use-for` property. The `use-for` property accepts the following values: `model-request`, `model-response`, `mcp-tool-request`, `mcp-tool-response`, and `*`.

=== Guardrail of similar text

The built-in link:_attachments/api/akka/javasdk/agent/SimilarityGuard.html[`SimilarityGuard`] evaluates the text by making a similarity search in a dataset of "bad examples". If the similarity exceeds a threshold, the result is flagged as blocked.

This is how to configure the `SimilarityGuard`:

[source,conf,indent=0]
----
akka.javasdk.agent.guardrails {
  "jailbreak guard" {
    class = "akka.javasdk.agent.SimilarityGuard"
    agents = ["planner-agent", "weather-agent"]
    category = JAILBREAK
    use-for = ["model-request"]
    threshold = 0.75
    bad-examples-resource-dir = "guardrail/jailbreak"
  }
}
----

Here, it's using predefined examples of jailbreak prompts in `guardrail/jailbreak`. Those have been incorporated from https://github.com/verazuo/jailbreak_llms/[https://github.com/verazuo/jailbreak_llms, window="new"], but you can define your own examples and place in a subdirectory of  `src/main/resources/`. All text files in the configured `bad-examples-resource-dir` are included in the similarity search.

This can be used for other things than jailbreak attempt detection.

== LLM evaluation

Evaluating AI quality is critical when refining prompts and model parameters. Without evaluation with realistic scenarios and data, you won’t know whether a change improves performance, breaks a use case, or has no impact at all.

Testing with generative AI is difficult no matter what you're using to implement it. Interactions with an LLM are not _deterministic_. In other words, you shouldn't expect to get the same answer twice for the same prompt. Because interactions with an LLM are not deterministic, traditional assertions don't work.

How can you write assertions for something like that? There are a ton of solutions, but most of them revolve around the idea that to verify an LLM's answer, you need another LLM. This pattern is often called "LLM-as-judge". You can get an answer from one agent, and then use another agent or model to review the session history and prompts to infer, with some level of confidence, if the agent behaved the way you want it to.

For instance, after a test run, you could send the session history to a powerful model like GPT-4 with a prompt like: "Based on the user's question about activities, did the agent correctly use the provided `getWeather` tool? Respond with only YES or NO."

There is a practice called **Agent evaluation** where developers will run single ad hoc tests or an automated battery of tests. You run your agent and then _evaluate_ the results based on a number of criteria like token usage, elapsed time, and the results of using other models to infer quality metrics like accuracy or confidence.

You can implement an LLM-as-judge evaluator as an Akka `Agent`. The result of the agent method should implement the link:_attachments/api/akka/javasdk/agent/EvaluationResult.html[`EvaluationResult`] interface. Essentially a boolean that tells if the input passed the evaluation criteria, and an explanation for the decision. These results are captured and included in metrics and traces.

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/evaluator/HumanVsAiEvaluator.java[HumanVsAiEvaluator.java]
----
include::example$doc-snippets/src/main/java/com/example/evaluator/HumanVsAiEvaluator.java[tag=all]
----
<1> It's an ordinary `Agent`
<2> It can have any type of request parameter
<3> The return type must implement `EvaluationResult`, but may also include more information
<4> Instructions of how to evaluate
<5> The method with return type implementing `EvaluationResult`
<6> The instructions are to use "correct" or "incorrect" in the label, and fail fast if that isn't followed by the model

Since the evaluator is an ordinary `Agent` you can call it with the component client in the same way as any other agent. For example, from a consumer of workflow state changes:

[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/AgentTeamEvaluatorConsumer.java[AgentTeamEvaluatorConsumer.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/AgentTeamEvaluatorConsumer.java[tag=class]
----
<1> Consumer of workflow state changes
<2> When there is a state change that is worth evaluating
<3> Call the evaluator agent with relevant parameters
<4> Additional logging, but metrics and traces are updated automatically from the evaluation result

This illustrates that evaluation happens asynchronously, in the background, to capture the results for analytics and later development improvements of prompts. However, the evaluators can also be part of the core agent workflow and thereby have a more immediate impact on the workflow. For example, if the outcome of some step in the workflow doesn't pass the evaluation it can refine the plan and iterate. In this case it's still good to capture the results in metrics and traces by using the `EvaluationResult`. The concrete, application specific, result may include more things than `EvaluationResult`, which can be used for adjusting the execution plan in the workflow.

NOTE: Evaluator agents have an associated cost and overhead since they typically use an LLM. You might want to enable them only in test environments and not for large scale production environments. You can xref:setup-and-dependency-injection.adoc#_disabling_components[disable consumers] that are calling evaluator agents.

An alternative approach is to not include evaluator agents in the deployed application at all, but only use them from integration tests with test data. This is a good way to capture regressions before deploying to the production environment. These tests would use the `TestKitSupport` and the `CoponentClient` to call the evaluators. Still using real LLM for the evaluator agents.

External evaluation products can be integrated with Akka by operating on the trace data that Akka captures and can export according to OpenTelemetry or OpenInference semantic conventions for AI.

=== Built-in evaluators

As shown above, it's easy to implement your own evaluator agents, but for convenience Akka provides a few built-in evaluators that you can use by calling them with the `CoponentClient`.

The model provider for these agents can be defined in a specific configuration for each agent, which by default is the same as the default model provider.

The system and user message prompts for these agents are loaded from a xref:#_using_dynamic_prompts_with_templates[`PromptTemplate` entity] with specific ids for each agent. Default prompts are used if these templates are not defined. The prompts can be initialized or updated with the xref:#_using_dynamic_prompts_with_templates[`PromptTemplate` entity].

==== Toxicity evaluator

link:_attachments/api/akka/javasdk/agent/evaluator/ToxicityEvaluator.html[`ToxicityEvaluator`] is an agent that acts as an LLM judge to evaluate if an AI response or other text is racist, biased, or toxic.

* Model provider configuration: `akka.javasdk.agent.evaluators.toxicity-evaluator.model-provider`
* System message prompt id: `toxicity-evaluator.system`
* User message prompt id: `toxicity-evaluator.user`

Default system message:

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/akka/javasdk/agent/evaluator/ToxicityEvaluator.java[tag=prompt]
----

==== Summarization evaluator

link:_attachments/api/akka/javasdk/agent/evaluator/SummarizationEvaluator.html[`SummarizationEvaluator`] is an agent that acts as an LLM judge to evaluate a summarization task.

* Model provider configuration: `akka.javasdk.agent.evaluators.summarization-evaluator.model-provider`
* System message prompt id: `summarization-evaluator.system`
* User message prompt id: `summarization-evaluator.user`

Default system message:

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/akka/javasdk/agent/evaluator/SummarizationEvaluator.java[tag=prompt]
----

==== Hallucination evaluator

link:_attachments/api/akka/javasdk/agent/evaluator/HallucinationEvaluator.html[`HallucinationEvaluator`] is an agent that acts as an LLM judge to evaluate whether an output contains information not available in the reference text given an input question.

* Model provider configuration: `akka.javasdk.agent.evaluators.hallucination-evaluator.model-provider`
* System message prompt id: `hallucination-evaluator.system`
* User message prompt id: `hallucination-evaluator.user`

Default system message:

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/akka/javasdk/agent/evaluator/HallucinationEvaluator.java[tag=prompt]
----

== Testing the agent

Testing agents built with Generative AI involves two complementary approaches: evaluating the quality of the non-deterministic model behavior and writing deterministic unit tests for the agent's and surrounding components' logic. Evaluations is described in xref:#_evaluating_ai_model_quality[], and here we will cover the deterministic testing.

=== Mocking responses from the model

For predictable and repeatable tests of your agent's business logic and component integrations, it's essential to use deterministic responses. This allows you to verify that your agent behaves correctly when it receives a known model output.

Use the `TestKitSupport` and the `CoponentClient` to call the components from the test. The `ModelProvider` of the agents can be replaced with link:_attachments/testkit/akka/javasdk/testkit/TestModelProvider.html[TestModelProvider], which provides ways to mock the responses without using the real AI model.

[source,java,indent=0]
.{sample-base-url}/multi-agent/src/test/java/demo/multiagent/application/AgentTeamWorkflowTest.java[AgentTeamWorkflowTest.java]
----
include::example$multi-agent/src/test/java/demo/multiagent/application/AgentTeamWorkflowTest.java[tag=class]
----
<1> Extend `TestKitSupport` to gain access to testing utilities for Akka components.
<2> Create one or more `TestModelProvider`. Using one per agent allows for distinct mock behaviors, while sharing one is useful for testing general responses.
<3> Use the settings of the `TestKit` to replace the agent's real `ModelProvider` with your test instance.
<4> For simple tests, define a single, fixed response that the mock model will always return.
<5> For more complex scenarios, define a response that is only returned if the user message matches a specific condition. This is useful for testing different logic paths within your agent.
<6> Call the components with the `componentClient`.

=== Mocked model in a deployed service

In some scenarios it can be useful to run the service deployed but without interacting with an actual agent. For example,
a load test that exercises the service with heavy load to verify scalability could quickly consume a large number of tokens
when the exact answer from the model is not very important, one or a few different predefined responses and responding with
a slight delay to simulate model processing time could be good enough.

It is possible to implement a custom model provider using `akka.javasdk.agent.ModelProvider.Custom`, such a mock provider
however, side steps quite a bit of the infrastructure involved in agent interactions, a more realistic mock model can be implemented
by building a separate Akka service with a single xref:http-endpoints.adoc[HTTP endpoint] mimicking the model endpoint and
configuring the deployed agentic service to use that.

Here is an example endpoint returning a static response over the OpenAI protocol:

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/api/MockOpenAI.java[MockOpenAI.java]
----
include::example$doc-snippets/src/main/java/com/example/api/MockOpenAI.java[tag=class]
----

For more elaborate scenarios, the mock model endpoint may have to parse the request to decide which hard coded answer out of a few
or to create a reply in a more dynamic fashion.

Deploying this service as `mock-openai` allows other services containing agents in the same xref:operations:projects/index.adoc[Akka project].
Using the deployed mock service from an agent in another service can be done with a config like this:

[source,hocon,indent=0]
.application.conf
----
akka.javasdk {
  agent {
    model-provider = openai

    openai {
      model-name = "gpt-4o-mini"
      base-url = "http://mock-openai" // <1>
    }
  }
}

----
1. The service name the mock was deployed as.

Note that you should use `http`, and not `https`, the connection will be encrypted with TLS, but that is handled by the platform.

=== Log model request and response

To see exactly what is sent to and received from the AI model, you can enable the following logger in `include-dev-loggers.xml`:

[source]
----
  <logger name="kalix.runtime.agent.AkkaLangChain4jHttpClient" level="TRACE"/>
----
