= Executing RAG queries

include::ROOT:partial$include.adoc[]

== Overview

In this step of the guide to building the _Ask Akka_ application, you'll be creating a class that wraps the OpenAI API and the MongoDB client API. It's this class that will provide the abstraction for the rest of the application to use when making RAG queries. You'll use Akka's `@Setup` to configure the dependency injection for this class.

== Prerequisites

include::ROOT:partial$cloud-dev-prerequisites.adoc[]

== Creating the Akka Agent class

We know we're going to be writing a utility that interacts with the LLM for us. Here the choice of how to accomplish this is far more subjective and based more on people's Java preferences than their knowledge of Akka. In this case, we've opted to put the logic behind the `AskAkkaAgent` class and supporting utilities.

The following is the basic shell of the class before we add RAG-specific code to it.


[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/agent/application/AskAkkaAgent.java[AskAkkaAgent.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/agent/application/AskAkkaAgent.java[tag=class]
----
<1> This is the _system prompt_. This will be sent along with context and history for each LLM call
<2> The `Knowledge` instance will be injected by the bootstrap setup class

Next we add the `createAssistant` method. This is almost entirely made up of `langchain4j` code and not specific to Akka. The purpose of this function is to use langchain4j's `AiServices` builder class to set up retrieval augmentation (MongoDB) and the chat model (Open AI).

Next we need a utility to form a bridge between langchain4j and Akka.

== Creating a streaming source

In the preceding code, we call `AkkaStreamUtils.toAkkaSource` on the result of `assistant.chat(userQuestion)`. This is a utility method that converts the stream of tokens returned by langchain4j's `chat` method into an Akka stream _source_. We do that so that any endpoint component (shown in the next guide) can stream meaningful results. The tokens get converted into meaningful results via an asynchronous _map_.

The code for this method delves into a bit of advanced Akka code in order to create a stream from a langchain4j object, but it's only a few lines of code without comments.

== Next steps
Next we'll create streaming endpoints that provide clients with real-time access to LLM output.

IMPORTANT: *Coming soon!* We are actively working on building and vetting this content. We will announce more content as it arrives.




