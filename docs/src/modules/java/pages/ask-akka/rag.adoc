= Executing RAG queries

include::ROOT:partial$include.adoc[]

== Overview

In this step of the guide to building the _Ask Akka_ application, you'll be creating a class that wraps the OpenAI API and the MongoDB client API. It's this class that will provide the abstraction for the rest of the application to use when making RAG queries. You'll use Akka's `@Setup` to configure the dependency injection for this class.

== Prerequisites

include::ROOT:partial$cloud-dev-prerequisites.adoc[]

== Updating the bootstrap

In the previous section we created a bootstrap class that set up dependency injection for the MongoDB client. This client needs to be injected into the indexing workflow to use MongoDB as the vector store. We can just add a few lines to the `createDependencyProvider` method to also create an instance of the class we'll build next, `AskAkkaAgent`

[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/Bootstrap.java[Bootstrap.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/Bootstrap.java[tag=agent]
----

== Creating the Akka Agent class

We know we're going to be writing a utility that interacts with the LLM for us. Here the choice of how to accomplish this is far more subjective and based more on people's Java preferences than their knowledge of Akka. In this case, we've opted to put the logic behind the `AskAkkaAgent` class and supporting utilities.

The following is the basic shell of the class before we add RAG-specific code to it.


[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/agent/application/AskAkkaAgent.java[AskAkkaAgent.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/agent/application/AskAkkaAgent.java[tag=class]
----
<1> This is the _system prompt_. This will be sent along with context and history for each LLM call
<2> The `MongoClient` instance will be injected by the boot strap setup class
<3> This function gets called after each LLM output stream finishes

Next we add the `createAssistant` method. This is almost entirely made up of `langchain4j` code and not specific to Akka. The purpose of this function is to use langchain4j's `AiServices` builder class to set up retrieval augmentation (MongoDB) and the chat model (Open AI).

[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/agent/application/AskAkkaAgent.java[AskAkkaAgent.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/agent/application/AskAkkaAgent.java[tag=rag]
----
<1> Use the Open AI embedding model with MongoDB Atlas as the embedding store
<2> Set the message history for this instance
<3> Plug everything together using `AiServices` from langchain4j
<4> We've received the full output stream from the LLM, so tell the session entity about it
<5> This is just a part of the stream so keep streaming to the original caller

Next we need a utility to form a bridge between langchain4j and Akka.

== Creating a streaming source

In the preceding code, we call `AkkaStreamUtils.toAkkaSource` on the result of `assistant.chat(userQuestion)`. This is a utility method that converts the stream of tokens returned by langchain4j's `chat` method into an Akka stream _source_. We do that so that any endpoint component (shown in the next guide) can stream meaningful results. The tokens get converted into meaningful results via an asynchronous _map_.

The code for this method delves into a bit of advanced Akka code in order to create a stream from a langchain4j object, but it's only a few lines of code without comments.

[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/agent/application/AkkaStreamUtils.java[AkkaStreamUtils.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/agent/application/AkkaStreamUtils.java[tag=func]
----
<1> Input is a langchain4j token stream, output is an Akka stream source 
<2> `Source.queue` builds a new source backed by a queue, this one has a max length of 10,000
<3> If we get tokens before we finish, we add them to the stream (via `offer`)
<4> If the token stream is finished, then we `offer` and then `complete`

== Next steps
Next we'll create streaming endpoints that provide clients with real-time access to LLM output.

IMPORTANT: *Coming soon!* We are actively working on building and vetting this content. We will announce more content as it arrives.




