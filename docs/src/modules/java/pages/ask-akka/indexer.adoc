= Knowledge indexing with a workflow

include::ROOT:partial$include.adoc[]

== Overview

The first step in building a RAG agent is _indexing_. Each time a user submits a query or prompt to the agent, the agent _retrieves_ relevant documents by performing a semantic search on a vector database. Before we can perform that search, we need to populate the vector database with all of the knowledge that we want to make available to the agent.

Populating the vector database by creating embeddings is the _indexing_ step. In this guide we're going to use an Akka workflow to manage the indexing of a large number of documents as a long-running process.

== Prerequisites

include::ROOT:partial$cloud-dev-prerequisites.adoc[]

You will also need a link:https://www.mongodb.com/atlas[Mongo DB Atlas] account. We'll be using the vector indexing capability of this database for the retrieval portion of the RAG flow. You can do all of the indexing necessary for this sample with a free account. Once you've created the account, make note of the secure connection string as you'll need it later.

If you are following along with each step rather than using the completed solution, then you'll need the code you wrote in the previous step.

== Updating the pom

We're going to use `langchain4j` for this sample, so add those dependencies to your Maven pom file. The full file should look like this when done:

[source, xml, indent=0]
.{sample-base-url}/ask-akka-agent/pom.xml[pom.xml]
----
include::example$ask-akka-agent/pom.xml[]
----

== Adding a workflow

In your code, add a new empty Java file at `src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java`. The imports section is large enough that we won't show it here (you can see it in the source code link).

Let's start with the outer shell of the workflow class (this won't compile yet as we haven't included the workflow definition).

[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[RagIndexingWorkflow.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[tag=shell]
----
<1> The workflow will maintain a list of files to process and a list of files already processed
<2> We treat the list of files as a queue

The workflow definition for the document indexer is surprisingly simple:
[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[RagIndexingWorkflow.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[tag=def]
----
<1> Define the only step in the workflow, `processing`
<2> Check if we have more work to do
<3> If there is more work, transition to `processing` again
<4> If there are no files pending, the workflow will _pause_

Because this workflow only ever transitions to and from the same state, it might help to think of it as a _recursive_ workflow. An interesting aspect of this workflow is that it never ends. If it runs out of files to process, then it simply pauses itself. We haven't coded it in this sample, but it would be fairly easy to add an endpoint that allowed a user to enqueue more files for the indexer and wake/unpause it.

The actual work of doing the indexing is in the `indexFile` function:

[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[RagIndexingWorkflow.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[tag=index]
----

and the `addSegment` function which calls `add` on the embedding store, committing the segment (aka _chunk_) to MongoDB Atlas:

[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[RagIndexingWorkflow.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[tag=add]
----
<1> Send the embedding segment to the vector database

Everything that we've done so far has been completely asynchronous. When the workflow starts (shown below), it builds the list of pending documents by walking the documents directory and adding each markdown (`*.md`) file it finds. You can find all of these documents in the sample folder `src/main/resources/flat-doc`.

[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[RagIndexingWorkflow.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[tag=start]
----
<1> A workflow must always transition to a state on startup

== Injecting the MongoDB client

If you've been following along, then you might be wondering how we inject an `embeddingStore` field into this workflow. This field is of type `MongoDbEmbeddingStore`, and to create an instance of that we need to inject a `MongoClient` to the workflow's constructor:
[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[RagIndexingWorkflow.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/indexer/application/RagIndexingWorkflow.java[tag=cons]
----
<1> Tweaking the parameters to the document splitter can affect the quality of semantic search results

The API endpoint to start the indexer creates an instance of the workflow through the standard `ComponentClient` function `forWorkflow`. To make the `MongoClient` instance available, we can use a bootstrap class that uses Akka's `@Setup` attribute:
[source, java, indent=0]
.{sample-base-url}/ask-akka-agent/src/main/java/akka/ask/Bootstrap.java[Bootstrap.java]
----
include::example$ask-akka-agent/src/main/java/akka/ask/Bootstrap.java[tag=all]
----

As you'll see in the next step in this guide, we'll add to this bootstrap to inject a service that does the actual LLM communication for us.

For now, we suggest that you play around with indexing and the kind of results you see in MongoDB. Parameters like the size of chunks can sometimes impact the reliability or quality of the semantic search results. There are also several other types of document splitters. Explore those and see how it impacts the index.

You can set the `OPENAI_API_KEY` environment variable something random as you don't need it yet. Use the connection URL provided to you by MongoDB Atlas and set the `MONGODB_ATLAS_URI` environment variable to that connection string.

== Next steps
Next we'll write a service (in the dependency injection sense, not the Akka sense) that does all of the asynchronous LLM communication work. We'll then put an API in front of it and be able to run queries against the _Ask Akka_ AI assistant!


