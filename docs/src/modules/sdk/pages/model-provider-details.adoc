= AI model provider configuration

include::ROOT:partial$include.adoc[]

Akka provides integration with several backend AI models. You are responsible for configuring the AI model provider for every agent you build, whether you do so with configuration settings or via code.

As discussed in the xref:agents.adoc#model[Configuring the model] section of the Agent documentation, supplying a model provider through code will override the model provider configured through `application.conf` settings. You can also have multiple model providers configured and then use the `fromConfig` method of the `ModelProvider` class to load a specific one.

This page provides a detailed list of all of the configuration values available to each provider. As with all Akka configuration, the model configuration is declared using the https://github.com/lightbend/config/blob/main/HOCON.md[HOCON] format.

== Definitions
The following are a few definitions that might not be familiar to you. Not all models support these properties, but when they do, their definition remains the same.

=== Temperature
A value from 0.0 to 1.0 that indicates the amount of randomness in the model output. Often described as controlling how "creative" a model can get. The lower the value, the more precise and strict you want the model to behave. The higher the value, the more you expect it to improvise and the less deterministic it will be.

=== top-p
This property refers to the "Nucleus sampling parameter." Controls text generation by only considering the most likely tokens whose cumulative probability
exceeds the threshold value. It helps balance between diversity and
quality of outputs—lower values (like 0.3) produce more focused,
predictable text while higher values (like 0.9) allow more creativity
and variation.

=== top-k
Top-k sampling limits text generation to only the k most probable
tokens at each step, discarding all other possibilities regardless
of their probability. It provides a simpler way to control randomness,
smaller k values (like 10) produce more focused outputs while larger
values (like 50) allow for more diversity.

=== max-tokens or max-completion-tokens
If this value is supplied and the model supports this property, then it will stop operations in mid flight if the token quota runs out. It's important to check _how_ the model counts tokens, as some may count differently. Be aware of the fact that this parameter name frequently varies from one provider to the next. Make sure you're using the right property name.

== Model configuration
The following is a list of all natively supported model configurations. Remember that if you don't see your model or model format here, you can always create your own custom configuration and still use all of the Agent-related components.

=== Anthropic

[cols="1m,1,4"]
|===
| Property | Type | Description

| provider 
| "anthropic" 
| Name of the provider. Must always be `anthropic`

| api-key
| String 
| The API key. Defaults to the value of the `ANTHROPIC_API_KEY` environment variable

| model-name
| String
| The name of the model to use. See vendor documentation for a list of available models

| base-url
| Url
| Optional override to the base URL of the API

| temperature
| Float
| Model randomness. The default is not supplied so check with the model documentation for default behavior

| top-p
| Float
| Nucleus sampling parameter

| top-k
| Integer
| Top-k sampling parameter

| max-tokens
| Integer
| Max token quota. Leave as –1 for model default

|===


See link:_attachments/api/akka/javasdk/agent/ModelProvider.Anthropic.html[`ModelProvider.Anthropic`] for programmatic settings.

=== Gemini

[cols="1m,1,4"]
|===
| Property | Type | Description

| provider 
| "googleai-gemini" 
| Name of the provider. Must always be `googleai-gemini`

| api-key
| String 
| The API key. Defaults to the value of the `GOOGLE_AI_GEMINI_API_KEY` environment variable

| model-name
| String
| The name of the model to use. See vendor documentation for a list of available models

| base-url
| Url
| Optional override to the base URL of the API

| temperature
| Float
| Model randomness. The default is not supplied so check with the model documentation for default behavior

| top-p
| Float
| Nucleus sampling parameter

| max-output-tokens
| Integer
| Max token _output_ quota. Leave as –1 for model default

|===

See link:_attachments/api/akka/javasdk/agent/ModelProvider.GoogleAIGemini.html[`ModelProvider.GoogleAIGemini`] for programmatic settings.

=== Hugging Face

[cols="1m,1,4"]
|===
| Property | Type | Description

| provider 
| "hugging-face" 
| Name of the provider. Must always be `hugging-face`

| access-token
| String
| The access token for authentication with the Hugging Face API

| model-id
| String
| The ID of the model to use. See vendor documentation for a list of available models

| base-url
| Url
| Optional override to the base URL of the API

| temperature
| Float
| Model randomness. The default is not supplied so check with the model documentation for default behavior

| top-p
| Float
| Nucleus sampling parameter

| max-new-tokens
| Integer
| Max number of tokens to generate (–1 for model default)

|===

See link:_attachments/api/akka/javasdk/agent/ModelProvider.HuggingFace.html[`ModelProvider.HuggingFace`] for programmatic settings.

=== Local AI

[cols="1m,1,4"]
|===
| Property | Type | Description

| provider 
| "local-ai" 
| Name of the provider. Must always be `local-ai`

| model-name
| String
| The name of the model to use. See vendor documentation for a list of available models

| base-url
| Url
| Optional override to the base URL of the API (default `http://localhost:8080/v1`)

| temperature
| Float
| Model randomness. The default is not supplied so check with the model documentation for default behavior

| top-p
| Float
| Nucleus sampling parameter

| max-tokens
| Integer
| Max number of tokens to generate (–1 for model default)

|===

See link:_attachments/api/akka/javasdk/agent/ModelProvider.LocalAI.html[`ModelProvider.LocalAI`] for programmatic settings.

=== Ollama

[cols="1m,1,4"]
|===
| Property | Type | Description

| provider 
| "ollama" 
| Name of the provider. Must always be `ollama`

| model-name
| String
| The name of the model to use. See vendor documentation for a list of available models

| base-url
| Url
| Optional override to the base URL of the API (default `http://localhost:11434`)

| temperature
| Float
| Model randomness. The default is not supplied so check with the model documentation for default behavior

| top-p
| Float
| Nucleus sampling parameter

|===

See link:_attachments/api/akka/javasdk/agent/ModelProvider.Ollama.html[`ModelProvider.Ollama`] for programmatic settings.

=== OpenAI

[cols="1m,1,4"]
|===
| Property | Type | Description

| provider 
| "openai" 
| Name of the provider. Must always be `openai`

| api-key
| String 
| The API key. Defaults to the value of the `OPENAI_API_KEY` environment variable

| model-name
| String
| The name of the model to use (e.g. "gpt-4" or "gpt-3.5-turbo"). See vendor documentation for a list of available models

| base-url
| Url
| Optional override to the base URL of the API

| temperature
| Float
| Model randomness. The default is not supplied so check with the model documentation for default behavior

| top-p
| Float
| Nucleus sampling parameter

| max-tokens
| Integer
| Max token quota. Leave as –1 for model default. Not supported by GPT-5, use max-completion-tokens instead.

| max-completion-tokens
| Integer
| Max token quota. Leave as –1 for model default

|===

See link:_attachments/api/akka/javasdk/agent/ModelProvider.OpenAi.html[`ModelProvider.OpenAi`] for programmatic settings.

== Default model configuration

The default model will be used if the agent doesn't specify another model.

You can define a default model in `application.conf`:

[source,json,indent=0]
.src/main/resources/application.conf
----
include::sdk:example$doc-snippets/src/main/resources/application.conf[tags=agent-model-config]
----

The `model-provider` property points to the name of another configuration section, in this case `akka.javasdk.agent.openai`. That configuration section contains the actual configuration for the model provider, according to the properties described in below xref:#_reference_configurations[].

Another example where we have selected `anthropic` with `claude-sonnet-4` as the default model provider:

[source,json,indent=0]
.src/main/resources/application.conf
----
include::sdk:example$doc-snippets/src/main/resources/application.conf[tags=agent-model-config2]
----

The API key can be defined with an environment variable, `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` in the above examples.

== Reference configurations
The following is a list of the various reference configurations for each of the AI models

Note that the following reference configurations are the default values, and you would typically only define the properties that you want to override, such as:

[source,hocon,indent=0]
----
akka.javasdk.agent.openai {
  model-name = "gpt-4o-mini"
}
----

You may also have to use a fallback to the reference configuration if you use a different configuration section:

[source,hocon,indent=0]
----
gpt-o3 = ${akka.javasdk.agent.openai}
gpt-o3 {
  model-name = "o3"
  max-completion-tokens = 200000
}
----


=== Anthropic
[source,hocon,indent=0]
----
include::example$akka-javasdk/src/main/resources/reference.conf[tag=anthropic]
----

=== Gemini
[source,hocon,indent=0]
----
include::example$akka-javasdk/src/main/resources/reference.conf[tag=googleai-gemini]
----

=== Hugging face
[source,hocon,indent=0]
----
include::example$akka-javasdk/src/main/resources/reference.conf[tag=hugging-face]
----

=== Local AI
[source,hocon,indent=0]
----
include::example$akka-javasdk/src/main/resources/reference.conf[tag=local-ai]
----

=== Ollama
[source,hocon,indent=0]
----
include::example$akka-javasdk/src/main/resources/reference.conf[tag=ollama]
----

=== OpenAI
[source,hocon,indent=0]
----
include::example$akka-javasdk/src/main/resources/reference.conf[tag=openai]
----

