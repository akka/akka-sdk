= Streaming responses

include::ROOT:partial$include.adoc[]

In AI chat applications, you've seen how responses are displayed word by word as they are generated. There are a few reasons for this. The first is that LLMs are _prediction_ engines. Each time a token (usually a word) is streamed to the response, the LLM will attempt to _predict_ the next word in the output. This causes the small delays between words.

The other reason why responses are streamed is that it can take a very long time to generate the full response, so the user experience is much better getting the answer as a live stream of tokens. To support this real-time user experience, the agent can stream the model response tokens to an endpoint. These tokens can then be pushed to the client using server-sent events (SSE).

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=stream-tokens]
----
<1> The method returns `StreamEffect` instead of `Effect<T>`.
<2> Use the `streamEffects()` builder.

Consuming the stream from an HTTP endpoint:

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=stream-endpoint]
----
<1> Use `tokenStream` of the component client, instead of `method`,
<2> and invoke it with `source` to receive a stream of tokens.
<3> Return the stream of tokens as a streaming HTTP response.

The returned stream is a `Source<String, NotUsed>`, i.e. the tokens are always text strings.

The granularity of a token varies by AI model, often representing a word or a short sequence of characters. To reduce the overhead of sending each token as a separate SSE, you can group multiple tokens together using the Akka streams `groupWithin` operator.

[source,java,indent=0]
----
include::example$doc-snippets/src/main/java/com/example/application/ActivityAgentMore.java[tag=stream-group]
----
<1> Group at most 20 tokens or within 100 milliseconds, whatever happens first.
<2> Concatenate the list of string into a single string.
<3> Return the stream of grouped tokens as a streaming HTTP response.

NOTE: Token streams are designed for direct agent calls from an endpoint. You can't use a token stream when you have an intermediate workflow between the endpoint and the agent.

