= LLM evaluation

include::ROOT:partial$include.adoc[]

Evaluating AI quality is critical when refining prompts and model parameters. Without evaluation with realistic scenarios and data, you wonâ€™t know whether a change improves performance, breaks a use case, or has no impact at all.

Testing with generative AI is difficult no matter what you're using to implement it. Interactions with an LLM are not _deterministic_. In other words, you shouldn't expect to get the same answer twice for the same prompt. Because interactions with an LLM are not deterministic, traditional assertions don't work.

How can you write assertions for something like that? There are a ton of solutions, but most of them revolve around the idea that to verify an LLM's answer, you need another LLM. This pattern is often called "LLM-as-judge". You can get an answer from one agent, and then use another agent or model to review the session history and prompts to infer, with some level of confidence, if the agent behaved the way you want it to.

For instance, after a test run, you could send the session history to a powerful model like GPT-4 with a prompt like: "Based on the user's question about activities, did the agent correctly use the provided `getWeather` tool? Respond with only YES or NO."

You run your agent and then _evaluate_ the results based on a number of criteria like token usage, elapsed time, and the results of using other models to infer quality metrics like accuracy or confidence.

You can implement an LLM-as-judge evaluator as an Akka `Agent`. The result of the agent method should implement the link:{attachmentsdir}/api/akka/javasdk/agent/EvaluationResult.html[`EvaluationResult`] interface. Essentially a boolean that tells if the input passed the evaluation criteria, and an explanation for the decision. These results are captured and included in metrics and traces.

[source,java,indent=0]
.{sample-base-url}/doc-snippets/src/main/java/com/example/evaluator/HumanVsAiEvaluator.java[HumanVsAiEvaluator.java]
----
include::example$doc-snippets/src/main/java/com/example/evaluator/HumanVsAiEvaluator.java[tag=all]
----
<1> It's an ordinary `Agent`
<2> It can have any type of request parameter
<3> The result from the model
<4> The return type must implement `EvaluationResult`, but may also include more information
<5> Instructions of how to evaluate
<6> The method with return type implementing `EvaluationResult`
<7> Transform the model result

In this example, we use one result representation from the model, and a slightly different as the response type. These could be the same, but the model might be more accurate when using text labels instead of boolean values. It's also good to include validation in that transformation.

Since the evaluator is an ordinary `Agent` you can call it with the component client in the same way as any other agent. For example, from a consumer of workflow state changes:

[source,java,indent=0]
.{sample-base-url}/multi-agent/src/main/java/demo/multiagent/application/AgentTeamEvaluatorConsumer.java[AgentTeamEvaluatorConsumer.java]
----
include::example$multi-agent/src/main/java/demo/multiagent/application/AgentTeamEvaluatorConsumer.java[tag=class]
----
<1> Consumer of workflow state changes
<2> When there is a state change that is worth evaluating
<3> Call the evaluator agent with relevant parameters
<4> Additional logging, but metrics and traces are updated automatically from the evaluation result

This illustrates that evaluation happens asynchronously, in the background, to capture the results for analytics and later development improvements of prompts. However, the evaluators can also be part of the core agent workflow and thereby have a more immediate impact on the workflow. For example, if the outcome of some step in the workflow doesn't pass the evaluation it can refine the plan and iterate. In this case it's still good to capture the results in metrics and traces by using the `EvaluationResult`. The concrete, application specific, result may include more things than `EvaluationResult`, which can be used for adjusting the execution plan in the workflow.

NOTE: Evaluator agents have an associated cost and overhead since they typically use an LLM. You might want to enable them only in test environments and not for large scale production environments. You can xref:setup-and-dependency-injection.adoc#_disabling_components[disable consumers] that are calling evaluator agents.

An alternative approach is to not include evaluator agents in the deployed application at all, but only use them from integration tests with test data. This is a good way to capture regressions before deploying to the production environment. These tests would use the `TestKitSupport` and the `ComponentClient` to call the evaluators. Still using real LLM for the evaluator agents.

External evaluation products can be integrated with Akka by operating on the trace data that Akka captures and can export according to OpenTelemetry or OpenInference semantic conventions for AI.

== Built-in evaluators

As shown above, it's easy to implement your own evaluator agents, but for convenience Akka provides a few built-in evaluators that you can use by calling them with the `ComponentClient`. The `AgentTeamEvaluatorConsumer` example above shows how to call the evaluator agents.

The model provider for these agents can be defined in a specific configuration for each agent, which by default is the same as the default model provider.

The system and user message prompts for these agents are loaded from a xref:sdk:agents/prompt.adoc[`PromptTemplate` entity] with specific ids for each agent. Default prompts are used if these templates are not defined. The prompts can be initialized or updated with the xref:sdk:agents/orchestrating.adoc[`PromptTemplate` entity].

=== Toxicity evaluator

link:{attachmentsdir}/api/akka/javasdk/agent/evaluator/ToxicityEvaluator.html[`ToxicityEvaluator`] is an agent that acts as an LLM judge to evaluate if an AI response or other text is racist, biased, or toxic.

* Model provider configuration: `akka.javasdk.agent.evaluators.toxicity-evaluator.model-provider`
* System message prompt id: `toxicity-evaluator.system`
* User message prompt id: `toxicity-evaluator.user`

Default system message:

[source,java,indent=0]
----
include::example$akka-javasdk/src/main/java/akka/javasdk/agent/evaluator/ToxicityEvaluator.java[tag=prompt]
----

=== Summarization evaluator

link:{attachmentsdir}/api/akka/javasdk/agent/evaluator/SummarizationEvaluator.html[`SummarizationEvaluator`] is an agent that acts as an LLM judge to evaluate a summarization task.

* Model provider configuration: `akka.javasdk.agent.evaluators.summarization-evaluator.model-provider`
* System message prompt id: `summarization-evaluator.system`
* User message prompt id: `summarization-evaluator.user`

Default system message:

[source,java,indent=0]
----
include::example$akka-javasdk/src/main/java/akka/javasdk/agent/evaluator/SummarizationEvaluator.java[tag=prompt]
----

=== Hallucination evaluator

link:{attachmentsdir}/api/akka/javasdk/agent/evaluator/HallucinationEvaluator.html[`HallucinationEvaluator`] is an agent that acts as an LLM judge to evaluate whether an output contains information not available in the reference text given an input question.

* Model provider configuration: `akka.javasdk.agent.evaluators.hallucination-evaluator.model-provider`
* System message prompt id: `hallucination-evaluator.system`
* User message prompt id: `hallucination-evaluator.user`

Default system message:

[source,java,indent=0]
----
include::example$akka-javasdk/src/main/java/akka/javasdk/agent/evaluator/HallucinationEvaluator.java[tag=prompt]
----

